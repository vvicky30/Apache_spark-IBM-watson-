{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200515142757-0000\nKERNEL_ID = 9632965a-2131-4d98-847b-aa6a9678a879\n--2020-05-15 14:28:00--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-05-15 14:28:00--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-05-15 14:28:01--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-05-15 14:28:01 (27.7 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "#as for dataframe df we alraedy crate Temparory view for writting sql-queries on dataframe df\n#as------df.createOrReplaceTempView('df')\n#here we selecting another column which is continous as SLr-mata-data energy value which is sufficient for implementing linear regression\ndf_energy=spark.sql(\"\"\"\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label ,class from df group by class\n\"\"\")\ndf_energy.createOrReplaceTempView('df_energy')"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+--------------+\n|             label|         class|\n+------------------+--------------+\n| 8959.680239829991| Use_telephone|\n| 9737.511232342687| Standup_chair|\n| 12542.96539897962|      Eat_meat|\n|13225.945637269193|     Getup_bed|\n|15003.269043778426|   Drink_glass|\n|14454.885091207056|    Pour_water|\n|10616.408809008817|     Comb_hair|\n|16537.370891408344|          Walk|\n|11082.626493751379|  Climb_stairs|\n|10261.338314274606| Sitdown_chair|\n|6783.4063714331605|   Liedown_bed|\n| 7173.493500380411|Descend_stairs|\n| 11785.39634462923|   Brush_teeth|\n| 6071.460120926432|      Eat_soup|\n+------------------+--------------+\n\n"
                }
            ],
            "source": "df_energy.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "#now let join this to the original dataframe df(by innerjoint) :\ndf_join=spark.sql(\"\"\"\nselect * from df inner join df_energy on df.class=df_energy.class\n\"\"\")\n"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+\n|  x|  y|  z|              source|      class|            label|      class|\n+---+---+---+--------------------+-----------+-----------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n+---+---+---+--------------------+-----------+-----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df_join.show()#here you can see that energy values of  several classes are inner jointed to the df by coreesponding class wise where its group by classes"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.regression import LinearRegression "
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler=VectorAssembler(inputCols=['x','y','z'],outputCol='features')\nnormalizer=Normalizer(inputCol='features',outputCol='features_norm',p=1.0)\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "lr= LinearRegression(maxIter = 10 , regParam=0.3 , elasticNetParam=0.8)"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Exception ignored in: <object repr() failed>\nTraceback (most recent call last):\n  File \"/opt/ibm/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n    SparkContext._active_spark_context._gateway.detach(self._java_obj)\nAttributeError: 'VectorAssembler' object has no attribute '_java_obj'\n"
                }
            ],
            "source": "from pyspark.ml import Pipeline\npipeline=Pipeline(stages=[vectorAssembler,normalizer,lr])\nmodel=pipeline.fit(df_join)\nprediction=model.transform(df_join)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\n|  x|  y|  z|              source|      class|            label|      class|        features|       features_norm|        prediction|\n+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,49.0,35.0]|[0.20754716981132...|12586.729735016828|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,49.0,35.0]|[0.20754716981132...|12586.729735016828|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,35.0]|[0.20183486238532...|12542.703337345756|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,35.0]|[0.20183486238532...|12542.703337345756|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,52.0,34.0]|[0.19626168224299...|12573.865911821156|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,51.0,34.0]|[0.20560747663551...|12541.076564471234|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,50.0,35.0]|[0.19047619047619...|12666.983895607029|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,34.0]|[0.20370370370370...|12526.401098580878|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,50.0,34.0]|[0.20754716981132...|12555.752030361591|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,51.0,35.0]|[0.20370370370370...|12557.378803236114|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,51.0,33.0]|[0.2,0.4857142857...|12572.239138946636|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,50.0,34.0]|[0.19230769230769...| 12650.68165684215|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,49.0,33.0]|[0.20388349514563...|12601.590070727349|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,49.0,33.0]|[0.20388349514563...|12601.590070727349|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,51.0,35.0]|[0.18867924528301...|12652.308429716672|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,49.0,34.0]|[0.17821782178217...|12760.286749213064|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[19.0,48.0,34.0]|[0.18811881188118...|12727.497401863142|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[16.0,53.0,34.0]|[0.15533980582524...|12796.514512132195|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,52.0,35.0]|[0.17142857142857...|12732.562590306872|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,51.0,32.0]|[0.17821782178217...|12698.331339902592|\n+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "prediction.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.03259100556263628"
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.stages[2].summary.r2\n#evaluating the linear regression model by r2-score"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#as above r2-score is not very much efficient but its worth for the illusration "
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "#now we illustrating the LogisticRegression\n\ndf.show()\n"
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [],
            "source": "#now splitting the dataset\nsplits=df.randomSplit([0.8,0.2])\ndf_train=splits[0]\ndf_test=splits[1]"
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+\n|  x|  y|  z|              source|         class|\n+---+---+---+--------------------+--------------+\n|  0| 11| 38|Accelerometer-201...| Sitdown_chair|\n|  0| 12| 39|Accelerometer-201...| Sitdown_chair|\n|  0| 15| 39|Accelerometer-201...|   Brush_teeth|\n|  0| 16| 31|Accelerometer-201...|     Getup_bed|\n|  0| 17| 36|Accelerometer-201...|   Brush_teeth|\n|  0| 23| 36|Accelerometer-201...|   Brush_teeth|\n|  0| 24| 35|Accelerometer-201...| Sitdown_chair|\n|  0| 25| 30|Accelerometer-201...|   Brush_teeth|\n|  0| 26| 15|Accelerometer-201...|  Climb_stairs|\n|  0| 26| 42|Accelerometer-201...|   Brush_teeth|\n|  0| 27| 31|Accelerometer-201...| Sitdown_chair|\n|  0| 27| 33|Accelerometer-201...|     Getup_bed|\n|  0| 27| 41|Accelerometer-201...|   Brush_teeth|\n|  0| 28| 28|Accelerometer-201...|   Brush_teeth|\n|  0| 28| 48|Accelerometer-201...|   Brush_teeth|\n|  0| 29| 25|Accelerometer-201...|     Getup_bed|\n|  0| 29| 25|Accelerometer-201...|  Climb_stairs|\n|  0| 29| 32|Accelerometer-201...|Descend_stairs|\n|  0| 29| 34|Accelerometer-201...|          Walk|\n|  0| 29| 37|Accelerometer-201...|   Brush_teeth|\n+---+---+---+--------------------+--------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df_train.show()#"
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+\n|  x|  y|  z|              source|         class|\n+---+---+---+--------------------+--------------+\n|  0| 10| 28|Accelerometer-201...|     Getup_bed|\n|  0| 25| 40|Accelerometer-201...|   Brush_teeth|\n|  0| 27| 37|Accelerometer-201...|   Brush_teeth|\n|  0| 27| 39|Accelerometer-201...|   Brush_teeth|\n|  0| 29| 17|Accelerometer-201...|     Getup_bed|\n|  0| 29| 38|Accelerometer-201...|   Brush_teeth|\n|  0| 29| 43|Accelerometer-201...|   Brush_teeth|\n|  0| 30| 36|Accelerometer-201...|  Climb_stairs|\n|  0| 30| 43|Accelerometer-201...|   Brush_teeth|\n|  0| 31| 17|Accelerometer-201...| Standup_chair|\n|  0| 32| 23|Accelerometer-201...|     Getup_bed|\n|  0| 32| 33|Accelerometer-201...|          Walk|\n|  0| 33| 27|Accelerometer-201...|Descend_stairs|\n|  0| 33| 31|Accelerometer-201...|Descend_stairs|\n|  0| 33| 35|Accelerometer-201...|          Walk|\n|  0| 33| 35|Accelerometer-201...|          Walk|\n|  0| 33| 38|Accelerometer-201...|Descend_stairs|\n|  0| 33| 38|Accelerometer-201...|Descend_stairs|\n|  0| 33| 43|Accelerometer-201...|   Brush_teeth|\n|  0| 33| 60|Accelerometer-201...|     Getup_bed|\n+---+---+---+--------------------+--------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df_test.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import OneHotEncoder,StringIndexer, VectorAssembler, Normalizer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml import Pipeline\n\nstringindexer = StringIndexer(inputCol='class', outputCol='label')\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import LogisticRegression"
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": "lr= LogisticRegression(maxIter=10, regParam=0.3 ,elasticNetParam=0.8)"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": "#now creating pipeline\npipeline=Pipeline(stages=[stringindexer ,vectorAssembler,normalizer,lr])\nmodel=pipeline.fit(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": "predictions=model.transform(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  x|  y|  z|              source|         class|label|       features|       features_norm|       rawPrediction|         probability|prediction|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  0| 11| 38|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,11.0,38.0]|[0.0,0.2244897959...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 12| 39|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,12.0,39.0]|[0.0,0.2352941176...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 15| 39|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,15.0,39.0]|[0.0,0.2777777777...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 16| 31|Accelerometer-201...|     Getup_bed|  1.0|[0.0,16.0,31.0]|[0.0,0.3404255319...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 17| 36|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,17.0,36.0]|[0.0,0.3207547169...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 23| 36|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,23.0,36.0]|[0.0,0.3898305084...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 24| 35|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,24.0,35.0]|[0.0,0.4067796610...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 25| 30|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,25.0,30.0]|[0.0,0.4545454545...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 26| 15|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,26.0,15.0]|[0.0,0.6341463414...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 26| 42|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,26.0,42.0]|[0.0,0.3823529411...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 27| 31|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,27.0,31.0]|[0.0,0.4655172413...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 27| 33|Accelerometer-201...|     Getup_bed|  1.0|[0.0,27.0,33.0]|     [0.0,0.45,0.55]|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 27| 41|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,41.0]|[0.0,0.3970588235...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 28| 28|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,28.0]|       [0.0,0.5,0.5]|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 28| 48|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,48.0]|[0.0,0.3684210526...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 29| 25|Accelerometer-201...|     Getup_bed|  1.0|[0.0,29.0,25.0]|[0.0,0.5370370370...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 29| 25|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,29.0,25.0]|[0.0,0.5370370370...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 29| 32|Accelerometer-201...|Descend_stairs| 10.0|[0.0,29.0,32.0]|[0.0,0.4754098360...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 29| 34|Accelerometer-201...|          Walk|  0.0|[0.0,29.0,34.0]|[0.0,0.4603174603...|[1.25784217625196...|[0.20706476253427...|       0.0|\n|  0| 29| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,37.0]|[0.0,0.4393939393...|[1.25784217625196...|[0.20706476253427...|       0.0|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "predictions.show()#jst look at it"
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": "eval=MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')#creating the instance for multiclassevaluator"
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.20706459974359406"
                    },
                    "execution_count": 76,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "eval.evaluate(predictions)#for train data"
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": "model=pipeline.fit(df_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [],
            "source": "predictions= model.transform(df_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.20475342450410491"
                    },
                    "execution_count": 81,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "eval.evaluate(predictions)#for the test data"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}