{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200515174526-0000\nKERNEL_ID = a866dfcf-63be-42d2-a327-535aa78999b8\n"
                },
                {
                    "data": {
                        "text/markdown": "# <span style=\"color:red\"><<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>></span>",
                        "text/plain": "<IPython.core.display.Markdown object>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-15 17:45:39--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-05-15 17:45:39--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-05-15 17:45:40--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.05s   \n\n2020-05-15 17:45:40 (18.8 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Since this is supervised learning, let\u2019s split our data into train (80%) and test (20%) set."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-------------+\n|  x|  y|  z|              source|        class|\n+---+---+---+--------------------+-------------+\n|  0| 11| 38|Accelerometer-201...|Sitdown_chair|\n|  0| 12| 39|Accelerometer-201...|Sitdown_chair|\n|  0| 15| 39|Accelerometer-201...|  Brush_teeth|\n|  0| 16| 31|Accelerometer-201...|    Getup_bed|\n|  0| 17| 36|Accelerometer-201...|  Brush_teeth|\n|  0| 23| 36|Accelerometer-201...|  Brush_teeth|\n|  0| 24| 35|Accelerometer-201...|Sitdown_chair|\n|  0| 25| 30|Accelerometer-201...|  Brush_teeth|\n|  0| 25| 40|Accelerometer-201...|  Brush_teeth|\n|  0| 26| 15|Accelerometer-201...| Climb_stairs|\n|  0| 26| 42|Accelerometer-201...|  Brush_teeth|\n|  0| 27| 31|Accelerometer-201...|Sitdown_chair|\n|  0| 27| 37|Accelerometer-201...|  Brush_teeth|\n|  0| 27| 39|Accelerometer-201...|  Brush_teeth|\n|  0| 27| 41|Accelerometer-201...|  Brush_teeth|\n|  0| 28| 28|Accelerometer-201...|  Brush_teeth|\n|  0| 29| 17|Accelerometer-201...|    Getup_bed|\n|  0| 29| 25|Accelerometer-201...|    Getup_bed|\n|  0| 29| 25|Accelerometer-201...| Climb_stairs|\n|  0| 29| 34|Accelerometer-201...|         Walk|\n+---+---+---+--------------------+-------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df_train.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Again, we can re-use our feature engineering pipeline"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we use LogisticRegression, a simple and basic linear classifier to obtain a classification performance baseline."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)#fitting train\nprediction = model.transform(df_test)#ttransformig test set as for prediction"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "If we look at the schema of the prediction dataframe we see that there is an additional column called prediction which contains the best guess for the class our model predicts."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- features_norm: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"
                }
            ],
            "source": "prediction.printSchema()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  x|  y|  z|              source|         class|label|       features|       features_norm|       rawPrediction|         probability|prediction|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  0| 10| 28|Accelerometer-201...|     Getup_bed|  1.0|[0.0,10.0,28.0]|[0.0,0.2631578947...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 27| 33|Accelerometer-201...|     Getup_bed|  1.0|[0.0,27.0,33.0]|     [0.0,0.45,0.55]|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 28| 48|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,48.0]|[0.0,0.3684210526...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 29| 32|Accelerometer-201...|Descend_stairs| 10.0|[0.0,29.0,32.0]|[0.0,0.4754098360...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 29| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,37.0]|[0.0,0.4393939393...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 29| 43|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,43.0]|[0.0,0.4027777777...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 30| 29|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,30.0,29.0]|[0.0,0.5084745762...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 30| 35|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,30.0,35.0]|[0.0,0.4615384615...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 30| 46|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,30.0,46.0]|[0.0,0.3947368421...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 31| 32|Accelerometer-201...| Standup_chair|  7.0|[0.0,31.0,32.0]|[0.0,0.4920634920...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 31| 32|Accelerometer-201...| Standup_chair|  7.0|[0.0,31.0,32.0]|[0.0,0.4920634920...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 31| 35|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,31.0,35.0]|[0.0,0.4696969696...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 32| 33|Accelerometer-201...| Standup_chair|  7.0|[0.0,32.0,33.0]|[0.0,0.4923076923...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 32| 36|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,32.0,36.0]|[0.0,0.4705882352...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 32| 44|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,32.0,44.0]|[0.0,0.4210526315...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 33| 31|Accelerometer-201...|Descend_stairs| 10.0|[0.0,33.0,31.0]|[0.0,0.515625,0.4...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 33| 38|Accelerometer-201...|Descend_stairs| 10.0|[0.0,33.0,38.0]|[0.0,0.4647887323...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 33| 41|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,33.0,41.0]|[0.0,0.4459459459...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 34| 29|Accelerometer-201...| Standup_chair|  7.0|[0.0,34.0,29.0]|[0.0,0.5396825396...|[1.25882473808995...|[0.20737737860732...|       0.0|\n|  0| 34| 30|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,34.0,30.0]|[0.0,0.53125,0.46...|[1.25882473808995...|[0.20737737860732...|       0.0|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "prediction.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let\u2019s evaluate performance by using a build-in functionality of Apache SparkML."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.20349567245167946"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nMulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction) "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "So we get 20% right. This is not bad for a baseline. Note that random guessing would give us only 7%. Of course we need to improve. You might have notices that we\u2019re dealing with a time series here. And we\u2019re not making use of that fact right now as we look at each training example only individually. But this is ok for now. More advanced courses like \u201cAdvanced Machine Learning and Signal Processing\u201d (https://www.coursera.org/learn/advanced-machine-learning-signal-processing/) will teach you how to improve accuracy to the nearly 100% by using algorithms like Fourier transformation or wavelet transformation. But let\u2019s skip this for now. In the following cell, please use the RandomForest classifier (you might need to play with the \u201cnumTrees\u201d parameter) in the code cell below. You should get an accuracy of around 44%. More on RandomForest can be found here:\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier\n"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\").fit(df)\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import RandomForestClassifier\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_norm\", numTrees=10)\n\nfrom pyspark.ml.feature import IndexToString\n#at the end we convert predicted labels to the corresponding string ones(good to observe for humans)\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=indexer.labels)#for 'indexer.labels' does'nt work untill you fit 'indexer' to whole data"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "pipeline = Pipeline(stages=[indexer, vectorAssembler,normalizer, rf, labelConverter])"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": "# Train model.  This also runs the indexer.\nmodel = pipeline.fit(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": "# Make predictions.\npredictions = model.transform(df_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------+-----+---------------+\n|predictedLabel|label|       features|\n+--------------+-----+---------------+\n|     Getup_bed|  1.0|[0.0,10.0,28.0]|\n|          Walk|  1.0|[0.0,27.0,33.0]|\n|     Getup_bed|  6.0|[0.0,28.0,48.0]|\n|          Walk| 10.0|[0.0,29.0,32.0]|\n|          Walk|  6.0|[0.0,29.0,37.0]|\n+--------------+-----+---------------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Test Error = 0.576651\n"
                }
            ],
            "source": "# Select (prediction, true label) and compute test error(total(1)-accuracy)\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))"
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "accuracy= 0.42334858065384096\n"
                }
            ],
            "source": "#accuracy:---\nprint(\"accuracy=\",accuracy)\n#as guessed above "
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RandomForestClassificationModel (uid=RandomForestClassifier_4e47bbc8901665e64fec) with 10 trees\n"
                }
            ],
            "source": "rfModel = model.stages[3]\nprint(rfModel) "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}